{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63386a80",
   "metadata": {},
   "source": [
    "# Predicción de likes en Instagram (PyTorch)\n",
    "Notebook de demostración que reutiliza los módulos `preprocess.py`, `train.py`, `predict.py` y `models.py` para:\n",
    "1) Preprocesar datos\n",
    "2) Entrenar el modelo multimodal\n",
    "3) Ejecutar predicciones sobre un post\n",
    "\n",
    "Asegúrate de tener creada/activada tu `venv` e instalar `requirements.txt` antes de correr las celdas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a02d9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Define rutas y dispositivo. Ajusta `DATA_DIR` si tu dataset está en otra ubicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54723b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from models import get_device\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DIR = Path('processed')\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = get_device()\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e6124a",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento\n",
    "Esta celda llama al script `preprocess.py` para generar:\n",
    "- `processed_data.pt`\n",
    "- `vectorizer.joblib`\n",
    "- `text_scaler.joblib`\n",
    "- `meta_scaler.joblib`\n",
    "\n",
    "Solo necesitas ejecutarla la primera vez o cuando cambien los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess.py --data_dir $DATA_DIR --out_dir $PROCESSED_DIR --max_features 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('data')\n",
    "folders = sorted([p for p in data_dir.iterdir() if p.is_dir()])\n",
    "samples: List[Dict] = []\n",
    "for folder in folders:\n",
    "    try:\n",
    "        sample = extract_sample(folder)\n",
    "        samples.append(sample)\n",
    "    except Exception as exc:  # pragma: no cover\n",
    "        print(f\"[WARN] skipping {folder}: {exc}\")\n",
    "        continue\n",
    "\n",
    "captions = [s[\"caption_clean\"] for s in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b130b7e",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento\n",
    "Entrena el modelo multimodal. Ajusta épocas, learning rate o división de validación según resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --processed $PROCESSED_DIR/processed_data.pt --epochs 5 --batch_size 1 --lr 1e-4 --val_split 0.2 --model_out $PROCESSED_DIR/model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b61509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as t\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"notebook\",                         # nombre ficticio del script\n",
    "    \"--processed\", \"processed/processed_data.pt\",\n",
    "    \"--epochs\", \"10\",\n",
    "    \"--lr\", \"1e-3\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--val_split\", \"0.2\",\n",
    "    \"--model_out\", \"processed/model_manual.pt\",\n",
    "]\n",
    "\n",
    "model, val_loader, device = t.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "total_abs_err, n = 0.0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        for sample in batch:\n",
    "            images = [img.to(device) for img in sample[\"images\"]]\n",
    "            text   = sample[\"text\"].to(device)\n",
    "            meta   = sample[\"meta\"].to(device)\n",
    "\n",
    "            target = torch.expm1(sample[\"target\"].to(device))\n",
    "\n",
    "            pred_log = model(images, text, meta)\n",
    "            pred = torch.expm1(pred_log)\n",
    "\n",
    "            total_abs_err += torch.abs(pred - target).item()\n",
    "            n += 1\n",
    "\n",
    "mae = total_abs_err / n\n",
    "print(f\"MAE (likes): {mae:.4f} con n={n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762e352",
   "metadata": {},
   "source": [
    "## 3. Predicción sobre un post\n",
    "Usa el modelo guardado para predecir likes de un post (carpeta con jpg + txt + json.xz).\n",
    "Cambia `POST_DIR` por la carpeta que quieras evaluar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_DIR = DATA_DIR / 'aashnashroff_969148_3000403601659402518_25980_65'\n",
    "!python predict.py --data_dir $POST_DIR --model $PROCESSED_DIR/model.pt --vectorizer $PROCESSED_DIR/vectorizer.joblib --text_scaler $PROCESSED_DIR/text_scaler.joblib --meta_scaler $PROCESSED_DIR/meta_scaler.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47c63d",
   "metadata": {},
   "source": [
    "## 4. Inspección rápida de un sample en memoria (opcional)\n",
    "Carga un sample ya procesado y revisa shapes/datos para entender el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import InstagramPostDataset\n",
    "\n",
    "ds = InstagramPostDataset(PROCESSED_DIR/'processed_data.pt', device=device)\n",
    "sample = ds[0]\n",
    "print('id:', sample['id'])\n",
    "print('n_imgs:', len(sample['images']))\n",
    "print('text_vec shape:', sample['text'].shape)\n",
    "print('meta_vec:', sample['meta'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
